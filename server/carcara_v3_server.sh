#!/bin/bash

# ================================
# ExecuÃ§Ã£o do DeepSeek-R1 671B com Multi-GPU (RHEL 9.4)
# ================================

# ðŸ”¹ Passo 1: (Opcional) Atualiza e instala dependÃªncias do sistema
# sudo dnf makecache && sudo dnf install -y \
#     cmake gcc gcc-c++ make wget git curl unzip python3 python3-pip python3-venv \
#     libcublas openblas-devel openssl-devel libcurl-devel libstdc++ libffi-devel xz-devel

# ðŸ”¹ Passo 2: (Opcional) Instala dependÃªncias do Python
# pip install --upgrade pip
# pip install huggingface_hub transformers torch torchvision torchaudio numpy scipy pandas tqdm

# ðŸ”¹ FunÃ§Ãµes de Download do Modelo
download_model_quantized_simple() {
    echo "ðŸ”¹ Baixando apenas o modelo quantizado..."
    python3 - <<EOF
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id="unsloth/DeepSeek-R1-GGUF",
    local_dir="DeepSeek-R1-GGUF",
    allow_patterns=["*UD-IQ1_S*"]
)
EOF
}

download_model_quantized_complete() {
    echo "ðŸ”¹ Baixando modelo DeepSeek-V3 quantizado e tokenizador..."
    python3 - <<EOF
import os
from huggingface_hub import snapshot_download
from transformers import AutoTokenizer

# Baixar shards do modelo Q4_K_M
snapshot_download(
    repo_id="unsloth/DeepSeek-V3-GGUF",
    local_dir="DeepSeek-V3-GGUF",
    allow_patterns=["DeepSeek-V3-Q4_K_M/*.gguf"],
    resume_download=True
)

# Baixar tokenizador se ainda nÃ£o existir
tokenizer_path = "DeepSeek-V3-GGUF/tokenizer"
if not os.path.exists(tokenizer_path):
    tokenizer = AutoTokenizer.from_pretrained("unsloth/DeepSeek-V3")
    tokenizer.save_pretrained(tokenizer_path)
EOF
}



# ðŸ”¹ FunÃ§Ã£o para Configurar ParÃ¢metros (todos os parÃ¢metros serÃ£o passados pelo main)
set_parameters() {
    # Os parÃ¢metros sÃ£o recebidos na seguinte ordem:
    # 1 - MODEL_DIR
    # 2 - TOKENIZER_PATH
    # 3 - VRAM_TOTAL (por GPU, em GB)
    # 4 - GPU_COUNT
    # 5 - TOTAL_LAYERS do modelo
    # 6 - LLAMA_CLI (caminho para o executÃ¡vel)
    MODEL_DIR="$1"
    TOKENIZER_PATH="$2"
    VRAM_TOTAL="$3"
    GPU_COUNT="$4"
    TOTAL_LAYERS="$5"
    LLAMA_CLI="$6"

    echo "ðŸ”¹ Configurando parÃ¢metros..."

    # Define caminho do modelo e calcula valores derivados
    MODEL_PATH=$(ls "$MODEL_DIR"/*-00001-of-*.gguf | head -n 1)
    RAM_TOTAL=$(awk '/MemTotal/ {printf "%.0f", $2/1024/1024}' /proc/meminfo)
    VRAM_MAX=$((VRAM_TOTAL * GPU_COUNT - 1))   # Reserva 5GB no total
    RAM_MAX=$((RAM_TOTAL * 98 / 100))            # Usa atÃ© 98% da RAM total

    GPU_LAYERS="$TOTAL_LAYERS"  # Todas as camadas na GPU
    DISK_LAYERS=0

    # ðŸ”¹ Verifica se o modelo e o tokenizador foram baixados corretamente
    if [[ ! -f "$MODEL_PATH" ]]; then
        echo "âŒ ERRO: Modelo GGUF nÃ£o encontrado em $MODEL_PATH!"
        exit 1
    fi

    if [[ ! -d "$TOKENIZER_PATH" ]]; then
        echo "âŒ ERRO: Tokenizador nÃ£o encontrado em $TOKENIZER_PATH!"
        exit 1
    fi

    # ðŸ”¹ Verifica se llama-cli estÃ¡ compilado no caminho esperado
    if [[ ! -f "$LLAMA_CLI" ]]; then
        echo "âŒ ERRO: 'llama-cli' nÃ£o encontrado! Compilando..."
        cd /scratch/dockvs/leon.costa/src/llama.cpp
        rm -rf build
        cmake -B build -DBUILD_SHARED_LIBS=OFF -DGGML_CUBLAS=ON
        cmake --build build --config Release -j$(nproc) --clean-first --target llama-cli
        cd -
    fi

    if [[ ! -f "$LLAMA_CLI" ]]; then
        echo "âŒ ERRO: Falha na compilaÃ§Ã£o do 'llama-cli'! O script serÃ¡ encerrado."
        exit 1
    fi

    # ðŸ”¹ ConfiguraÃ§Ãµes CUDA e de MemÃ³ria
    export GGML_CUDA_MALLOC_ASYNC=1
    export GGML_CUDA_PEER_MAX_BATCH_SIZE=1024  # ComunicaÃ§Ã£o eficiente entre GPUs
    export GGML_USE_MLOCK=0  # Permite uso de SWAP (RAM nÃ£o bloqueada)
    export GGML_USE_MMAP=1   # Ativa cache no HD para camadas que nÃ£o couberem na RAM

    # ðŸ”¹ Define tokens especiais do DeepSeek-R1
    export LLAMA_TOKEN_BOS=0        # Token de inÃ­cio
    export LLAMA_TOKEN_EOS=1        # Token de fim
    export LLAMA_TOKEN_PAD=128815   # Token de padding
    export LLAMA_TOKEN_LF=201       # Token de nova linha
}

main() {
    echo "ðŸ”¹ Iniciando a execuÃ§Ã£o do script..."

    # ðŸ”¹ Escolha o tipo de download: Simple ou Complete
    #download_model_quantized_complete  
    
    # Pode ser alterado para download_model_quantized_simple
    # Caso altere o mÃ©todo de quantizaÃ§Ã£o, atualize tambÃ©m o sufixo de MODEL_DIR
    MODEL_DIR="DeepSeek-V3-0324-GGUF/DeepSeek-V3-0324-UD-Q2_K_XL/"   # DiretÃ³rio do modelo
    TOKENIZER_PATH="DeepSeek-V3-GGUF/tokenizer"             # DiretÃ³rio do tokenizador
    VRAM_TOTAL=80                                          # VRAM por GPU (em GB)
    GPU_COUNT=4                                            # NÃºmero de GPUs disponÃ­veis
    TOTAL_LAYERS=62                                        # Total de camadas do modelo
    LLAMA_CLI="/scratch/dockvs/leon.costa/src/llama.cpp/build/bin/llama-cli"  # Caminho para o executÃ¡vel

    # ðŸ”¹ Configura os parÃ¢metros chamando a funÃ§Ã£o com os argumentos definidos
    set_parameters "$MODEL_DIR" "$TOKENIZER_PATH" "$VRAM_TOTAL" "$GPU_COUNT" "$TOTAL_LAYERS" "$LLAMA_CLI"

    echo "ðŸ”¹ Executando DeepSeek-R1 671B em MULTI-GPU..."
    echo "ðŸ“Œ ConfiguraÃ§Ã£o de memÃ³ria:"
    echo "   - GPUs disponÃ­veis: ${GPU_COUNT}"
    echo "   - Total de camadas carregadas na GPU: ${GPU_LAYERS}"
    echo "   - Camadas na RAM/HD: ${DISK_LAYERS}"
    echo "   - VRAM alocada (total): ${VRAM_MAX}GB"
    echo "   - RAM disponÃ­vel: ${RAM_MAX}GB"

    START_TIME=$(date +%s)

    CUDA_VISIBLE_DEVICES=0,1,2,3 ./src/llama.cpp/build/bin/llama-server \
        --model "$MODEL_PATH" \
        --flash_attn \
        --cache-type-k q8_0 \
        --threads -1 \
        --ctx-size 16384 \
        --predict 4096 \
        --temp 0.1 \
        --top-k 40 \
        --top-p 0.9 \
        --min-p 0.1 \
        --batch-size 1024 \
        --n-gpu-layers "$GPU_LAYERS" \
        --split-mode layer \
        --host 0.0.0.0 \
        --port 8080 \
        --parallel 2 \
        --cont-batching \
        --threads-http 2 \
        --mlock \
        --no-webui \
        --timeout 180 \
        --metrics \
        --seed 420 \
        --alias carcara-v3-0324 \
        --api-key <ADICIONE_API_AQUI>
        #--chat-template-file system_prompt_template.jinja
        # --prompt "<|User|> Insira seu prompt abaixo! <|Assistant|>"

    END_TIME=$(date +%s)
    EXEC_TIME=$((END_TIME - START_TIME))

    echo "âœ… InferÃªncia concluÃ­da em ${EXEC_TIME} segundos!"
}

# ðŸ”¹ Inicia a execuÃ§Ã£o do script
main
